{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Training and Test datasets\n",
    "train_data = pd.read_csv('training_data.txt', sep=\"\\t\")\n",
    "test_data = pd.read_csv('testing_data.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the non-linear transformation for training and testing datasets\n",
    "train_data['x1_sq'] = train_data['x1']**2\n",
    "train_data['x2_sq'] = train_data['x2']**2\n",
    "train_data['x1_x2'] = train_data['x1']*train_data['x2']\n",
    "train_data['mod_diff'] = np.absolute(train_data['x1'] - train_data['x2'])\n",
    "train_data['mod_sum'] = np.absolute(train_data['x1'] + train_data['x2'])\n",
    "test_data['x1_sq'] = test_data['x1']**2\n",
    "test_data['x2_sq'] = test_data['x2']**2\n",
    "test_data['x1_x2'] = test_data['x1']*test_data['x2']\n",
    "test_data['mod_diff'] = np.absolute(test_data['x1'] - test_data['x2'])\n",
    "test_data['mod_sum'] = np.absolute(test_data['x1'] + test_data['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the independent and dependent variables and splitting the datasets into training, testing and \n",
    "# validation sets\n",
    "X_train = train_data.loc[0:24, train_data.columns != 'y']\n",
    "y_train = train_data.loc[0:24, train_data.columns == 'y']\n",
    "X_val = train_data.loc[25: , train_data.columns != 'y']\n",
    "y_val = train_data.loc[25: , train_data.columns == 'y']\n",
    "X_test = test_data.loc[:, test_data.columns != 'y']\n",
    "y_test = test_data.loc[:, test_data.columns == 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting a column of ones in the train, test and validation datasets\n",
    "X_train.insert(0,\"ones\",np.ones((X_train.shape[0],1)),True)\n",
    "X_val.insert(0,\"ones\",np.ones((X_val.shape[0],1)),True)\n",
    "X_test.insert(0,\"ones\",np.ones((X_test.shape[0],1)),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to compute the errors on a set different than the training set\n",
    "def pred_error_gen_design_matrix(X1,y1,X2,y2,k):\n",
    "    \"\"\"\n",
    "    Inputs-\n",
    "    X1: training design matrix (pdf, N_train x (d+1)) (already contains the ones column)\n",
    "    y1: response vector for the cases in training dataset (pdf, N_train x 1)\n",
    "    X2: design matrix to compute the error for the trained model(N_val x (d+1)) (pdf,already contains the ones column)\n",
    "    y2: response vector for the cases in X2 (pdf, N_val x 1)\n",
    "    k: Number of features to select out of the design matrix (must be greater than or equal to 1)\n",
    "    \n",
    "    Output-\n",
    "    Gives the validation error for the trained model for a given k\n",
    "    \"\"\"\n",
    "    # Converting the pandas dataframes into tractable numpy arrays\n",
    "    data_train = X1.values\n",
    "    resp_train = y1.values\n",
    "    data_comp_error = X2.values\n",
    "    resp_comp_error = y2.values\n",
    "    \n",
    "    # Fitting the regression model\n",
    "    a = np.linalg.inv(np.matmul(np.transpose(data_train[:,0:k+1]),data_train[:,0:k+1]))\n",
    "    b = np.dot(np.transpose(data_train[:,0:k+1]),resp_train.reshape(-1,1))\n",
    "    final_weight = np.dot(a,b)\n",
    "    \n",
    "    # Computing the validation error\n",
    "    y_pred = np.dot(data_comp_error[:,0:k+1],final_weight)\n",
    "    err_vec = (y_pred.reshape(-1,))*((resp_comp_error).reshape(-1,))\n",
    "    pred_error = np.sum(err_vec < 0)/data_comp_error.shape[0]\n",
    "    \n",
    "    return pred_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation error after learning the model from the training set is found for k =  6\n",
      "Minimum test error after learning the model from the training set is found for k =  7\n",
      "Minimum error on the training set after learning the model from the validation set is found for k =  6\n",
      "Minimum test error after learning the model from the validation set is found for k =  6\n"
     ]
    }
   ],
   "source": [
    "# Computing the errors for different design sets\n",
    "train_val_error_list = []\n",
    "train_test_error_list = []\n",
    "val_train_error_list = []\n",
    "val_test_error_list = []\n",
    "k = np.arange(3,8)\n",
    "for i in k:\n",
    "    pred_error = pred_error_gen_design_matrix(X_train,y_train,X_val,y_val,i)\n",
    "    train_val_error_list.append(pred_error)\n",
    "    pred_error = pred_error_gen_design_matrix(X_train,y_train,X_test,y_test,i)\n",
    "    train_test_error_list.append(pred_error)\n",
    "    pred_error = pred_error_gen_design_matrix(X_val,y_val,X_train,y_train,i)\n",
    "    val_train_error_list.append(pred_error)\n",
    "    pred_error = pred_error_gen_design_matrix(X_val,y_val,X_test,y_test,i)\n",
    "    val_test_error_list.append(pred_error)\n",
    "print(\"Minimum validation error after learning the model from the training set is found for k = \",k[train_val_error_list.index(min(train_val_error_list))])\n",
    "print(\"Minimum test error after learning the model from the training set is found for k = \",k[train_test_error_list.index(min(train_test_error_list))])\n",
    "print(\"Minimum error on the training set after learning the model from the validation set is found for k = \",k[val_train_error_list.index(min(val_train_error_list))])\n",
    "print(\"Minimum test error after learning the model from the validation set is found for k = \",k[val_test_error_list.index(min(val_test_error_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 0.5, 0.2, 0.0, 0.1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42, 0.416, 0.184, 0.084, 0.072]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28, 0.36, 0.2, 0.08, 0.12]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_train_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.396, 0.388, 0.284, 0.192, 0.196]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test_error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above error vectors we can draw following conclusions:\n",
    "<br>\n",
    "- <font size=\"3\">When we train our model on the training set and compute the error on the validation set which only has 10 points we cannot get a good estimate of Eout. This is reflected in the fact that even though the validation error when we select k features is observed to be 0 the error on the test set is observed to be 0.084. Also, the errors on the test set are not in alignment with the ones observed on the validation set, this is expected.</font>  \n",
    "<br>  \n",
    "- <font size=\"3\">When we reverse the roles of training and validation set we observe that Eouts are on the higher side for a complex set of features when compared to the usual training scenario but we are tracking Eout pretty closely through the validation window (which in this case is the training set). This is due to the fact that as number of examples have increased the variance in estimating Eout has reduced considerably but since the number of training examples have also reduced we are estimating a poor quantity.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

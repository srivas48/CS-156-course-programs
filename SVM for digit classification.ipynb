{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from statistics import mean \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train SVM on a model and compute the error on any specified dataset\n",
    "def SVM_analysis(data_training,data_testing,plus_one_label,c,q,kernel_type,gamma_val,coef0_val,minus_one_label=10):\n",
    "    \"\"\"\n",
    "    Inputs-\n",
    "    data_training: A Pandas dataframe of Nx3 to be used as a training set where the first column is the specific digit, second column\n",
    "                   is the intensity and third is symmetry\n",
    "    data_testing: A Pandas dataframe of Nx3 to be used as a test set where the first column is the specific digit, second column\n",
    "                   is the intensity and third is symmetry\n",
    "    plus_one_label: digit to be assigned the label +1\n",
    "    minus_one_label: digit to be assigned -1 label, only valid if we are doing one digit versus another one\n",
    "                     with all others being disregarded (by default it is 10)\n",
    "    c: penalty for violation (a scalar)\n",
    "    q: Degree of the polynomial Kernel (a scalar)\n",
    "    kernel_type: can be any of the types accepted by scikit learn (a string)\n",
    "    gamma_val: see the scikit learn documentation (https://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
    "    coef0_val: see the scikit learn documentation\n",
    "    \n",
    "    Output-\n",
    "    error_SVM - computes the error of the model on the dataset specified for testing\n",
    "    no_of_support_vectors - intuitive\n",
    "    \"\"\"\n",
    "    data_training['resp_label'] = np.where(data_training['digit']==plus_one_label, 1, -1)\n",
    "    data_testing['resp_label'] = np.where(data_testing['digit']==plus_one_label, 1, -1)\n",
    "    if 0<=minus_one_label<10:\n",
    "        data_training = data_training.loc[(data_training['digit']==plus_one_label) | (data_training['digit']==minus_one_label)]\n",
    "        data_testing = data_testing.loc[(data_testing['digit']==plus_one_label) | (data_testing['digit']==minus_one_label)] \n",
    "    labelled_training_data = data_training.copy()\n",
    "    labelled_testing_data = data_testing.copy()\n",
    "    clf = svm.SVC(C=c, degree=q, kernel=kernel_type,gamma=gamma_val,coef0=coef0_val)\n",
    "    # https://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "    train_values = labelled_training_data.values\n",
    "    test_values = labelled_testing_data.values\n",
    "    svm_fit = clf.fit(train_values[:,1:3], (train_values[:,3]).reshape(-1,))\n",
    "    pred_svm = (svm_fit.predict(test_values[:,1:3])).reshape(-1,1)\n",
    "    error_SVM = np.mean(pred_svm!=((test_values[:,3]).reshape(-1,1)))\n",
    "    no_of_support_vectors = (svm_fit.support_vectors_).shape[0]\n",
    "    return (error_SVM,no_of_support_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs k cross validation for SVM with binary classification error(as discussed in the course) and for any kernel \n",
    "def cross_validation_for_C(data_training,plus_one_label,c,q,nsims,kernel_type,gamma_val,coef0_val,kfold,minus_one_label=10):\n",
    "    \"\"\"\n",
    "    Inputs-\n",
    "    data_training: A Pandas dataframe of Nx3 to be used as a training set where the first column is the specific digit, second column\n",
    "                   is the intensity and third is symmetry\n",
    "    plus_one_label: digit to be assigned the label +1\n",
    "    minus_one_label: digit to be assigned -1 label, only valid if we are doing one digit versus another one\n",
    "                     with all others being disregarded (by default it is 10)\n",
    "    c: penalty for violation (a list)\n",
    "    q: Degree of the polynomial Kernel (a scalar)\n",
    "    nsims: number of simulations (a scalar)\n",
    "    kernel_type: can be any of the types accepted by scikit learn (a string)\n",
    "    gamma_val: see the scikit learn documentation (https://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
    "    coef0_val: see the scikit learn documentation\n",
    "    kfold: number of folds for cross validation\n",
    "    \n",
    "    Output-\n",
    "    dist_c - gives the number of times each value was selected in \"c\" (a list)\n",
    "    avg_err_cv - Average cross validation error for the finally selected model \n",
    "    \"\"\"\n",
    "    data_training['resp_label'] = np.where(data_training['digit']==plus_one_label, 1, -1)\n",
    "    if 0<=minus_one_label<10:\n",
    "        data_training = data_training.loc[(data_training['digit']==plus_one_label) | (data_training['digit']==minus_one_label)]\n",
    "    labelled_training_data = data_training.copy()\n",
    "    train_values = labelled_training_data.values\n",
    "    selected_val_c = []\n",
    "    dist_c = []\n",
    "    for i in range(nsims):\n",
    "        Ecv_penalty = []\n",
    "        for penalty in c:\n",
    "            clf = svm.SVC(C=penalty, degree=q, kernel=kernel_type,gamma=gamma_val,coef0=coef0_val) \n",
    "            kf = KFold(n_splits=kfold,shuffle=True)\n",
    "            X = train_values[:,1:3]\n",
    "            y = train_values[:,3]\n",
    "            error_svm = []\n",
    "            for folds in range(kfold):\n",
    "                for a,b in kf.split(X):\n",
    "                    X1, X2 = X[a], X[b]\n",
    "                    y1, y2 = y[a], y[b]\n",
    "                svm_fit = clf.fit(X1, y1.reshape(-1,))\n",
    "                pred_svm = (svm_fit.predict(X2)).reshape(-1,1)\n",
    "                error_svm.append(np.mean(pred_svm!=(y2.reshape(-1,1))))\n",
    "            Ecv_penalty.append(mean(error_svm))\n",
    "        selected_val_c.append(c[Ecv_penalty.index(min(Ecv_penalty))])\n",
    "    \n",
    "    # Generating the Distribution vector for selected c-values\n",
    "    for j in c:\n",
    "        dist_c.append(selected_val_c.count(j))\n",
    "        \n",
    "    # Computing the average cross validation error for the final selected model over 100 iterations\n",
    "    final_c = c[dist_c.index(max(dist_c))]\n",
    "    Ecv_sims = []\n",
    "    for k in range(nsims):\n",
    "        clf = svm.SVC(C=penalty, degree=q, kernel=kernel_type,gamma=gamma_val,coef0=coef0_val) \n",
    "        kf = KFold(n_splits=kfold,shuffle=True)\n",
    "        X = train_values[:,1:3]\n",
    "        y = train_values[:,3]\n",
    "        error_svm = []\n",
    "        for folds in range(kfold):\n",
    "            for a,b in kf.split(X):\n",
    "                X1, X2 = X[a], X[b]\n",
    "                y1, y2 = y[a], y[b]\n",
    "            svm_fit = clf.fit(X1, y1.reshape(-1,))\n",
    "            pred_svm = (svm_fit.predict(X2)).reshape(-1,1)\n",
    "            error_svm.append(np.mean(pred_svm!=(y2.reshape(-1,1))))\n",
    "        Ecv_sims.append(mean(error_svm))\n",
    "    return (dist_c,mean(Ecv_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Digit Training and Test datasets\n",
    "train_data = pd.read_csv('digit_identification_training.txt', sep=\"\\t\")\n",
    "test_data = pd.read_csv('digit_identification_testing.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum in sample error among even digits is observed for :  0\n",
      "The minimum in sample error among odd digits is observed for :  1\n",
      "The difference between the number of support vectors based on the models selected in Problem 2 and 3 is:  1879\n"
     ]
    }
   ],
   "source": [
    "# Problem 2\n",
    "i = range(0,9,2)\n",
    "in_sample_error_even = []\n",
    "no_support_vectors_even = []\n",
    "for j in i:\n",
    "    mod_res_even = SVM_analysis(train_data,train_data,j,0.01,2,'poly',1,0)\n",
    "    in_sample_error_even.append(mod_res_even[0])\n",
    "    no_support_vectors_even.append(mod_res_even[1])\n",
    "    \n",
    "print(\"The maximum in sample error among even digits is observed for : \", i[in_sample_error_even.index(max(in_sample_error_even))])\n",
    "\n",
    "# Problem 3\n",
    "k = range(1,8,2)\n",
    "in_sample_error_odd = []\n",
    "no_support_vectors_odd = []\n",
    "for j in k:\n",
    "    mod_res_odd = SVM_analysis(train_data,train_data,j,0.01,2,'poly',1,0)\n",
    "    in_sample_error_odd.append(mod_res_odd[0])\n",
    "    no_support_vectors_odd.append(mod_res_odd[1])\n",
    "    \n",
    "print(\"The minimum in sample error among odd digits is observed for : \", k[in_sample_error_odd.index(min(in_sample_error_odd))])\n",
    "\n",
    "# Problem 4\n",
    "print(\"The difference between the number of support vectors based on the models selected in Problem 2 and 3 is: \",no_support_vectors_even[0] - no_support_vectors_odd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error vector for second degree polynomial kernel is:  [0.010249839846252402, 0.004484304932735426, 0.004484304932735426, 0.004484304932735426, 0.003843689942344651]\n",
      "Out of sample error vector for second degree polynomial kernel is:  [0.01650943396226415, 0.01650943396226415, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886]\n",
      "Number of support vectors for second degree polynomial kernel:  [244, 80, 34, 24, 24]\n",
      "In sample error vector for fifth degree polynomial kernel is:  [0.005124919923126201, 0.004484304932735426, 0.005124919923126201, 0.004484304932735426, 0.004484304932735426]\n",
      "Out of sample error vector for fifth degree polynomial kernel is:  [0.01650943396226415, 0.01650943396226415, 0.01650943396226415, 0.018867924528301886, 0.01650943396226415]\n",
      "Number of support vectors for fifth degree polynomial kernel is:  [27, 26, 27, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "# Problem 5 and Problem 6: 1 vs 5 classifier\n",
    "C = [0.0001,0.001,0.01,0.1,1]\n",
    "\n",
    "# q=2 case\n",
    "no_c_support_vectors = []\n",
    "Ein_c = []\n",
    "Eout_c = []\n",
    "for penalty in C:\n",
    "    mod_res_c_train = SVM_analysis(train_data,train_data,1,penalty,2,'poly',1,0,5)\n",
    "    mod_res_c_test = SVM_analysis(train_data,test_data,1,penalty,2,'poly',1,0,5)\n",
    "    no_c_support_vectors.append(mod_res_c_train[1])\n",
    "    Ein_c.append(mod_res_c_train[0])\n",
    "    Eout_c.append(mod_res_c_test[0])\n",
    "print(\"In sample error vector for second degree polynomial kernel is: \",Ein_c)\n",
    "print(\"Out of sample error vector for second degree polynomial kernel is: \", Eout_c)\n",
    "print(\"Number of support vectors for second degree polynomial kernel: \", no_c_support_vectors)\n",
    "\n",
    "# q=5 case\n",
    "no_c_support_vectors5 = []\n",
    "Ein_c5 = []\n",
    "Eout_c5 = []\n",
    "for penalty in C:\n",
    "    mod_res_c_train = SVM_analysis(train_data,train_data,1,penalty,5,'poly',1,0,5)\n",
    "    mod_res_c_test = SVM_analysis(train_data,test_data,1,penalty,5,'poly',1,0,5)\n",
    "    no_c_support_vectors5.append(mod_res_c_train[1])\n",
    "    Ein_c5.append(mod_res_c_train[0])\n",
    "    Eout_c5.append(mod_res_c_test[0])\n",
    "print(\"In sample error vector for fifth degree polynomial kernel is: \",Ein_c5)\n",
    "print(\"Out of sample error vector for fifth degree polynomial kernel is: \", Eout_c5)\n",
    "print(\"Number of support vectors for fifth degree polynomial kernel is: \", no_c_support_vectors5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the usage of KFold function to implement cross validation with the binary classification error\n",
    "# as specified in the course. Scikit Learn doesn't enables it automatically\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[9,10],[11,12],[13,14],[15,16]])\n",
    "y = np.array([1, -1, 1, 1,1,-1,1,-1])\n",
    "kf = KFold(n_splits=4,shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "for X1,X2 in kf.split(X):\n",
    "    X_train,X_test = X[X1],X[X2]\n",
    "    y_train, y_test = y[X1], y[X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of C as per cross-validation is:,  0.001\n",
      "The average cross validation error for the best value of penalty parameter is:  0.0048397435897435895\n"
     ]
    }
   ],
   "source": [
    "# Problem 7 and Problem 8: Cross Validation\n",
    "C = [0.0001,0.001,0.01,0.1,1]\n",
    "out_cv = cross_validation_for_C(train_data,1,C,2,100,'poly',1,0,10,5)\n",
    "c_vec = out_cv[0]\n",
    "print(\"The best value of C as per cross-validation is:, \",C[c_vec.index(max(c_vec))])\n",
    "print(\"The average cross validation error for the best value of penalty parameter is: \",out_cv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum in sample error is observed for C= :  10000000.0\n",
      "The minimum out of sample error is observed for C= :  100\n"
     ]
    }
   ],
   "source": [
    "# Problem 9 and Problem 10: Using RBF Kernel for 1 vs 5 classifier\n",
    "C = [0.01,1,100,10e4,10e6]\n",
    "in_sample_error_rbf = []\n",
    "out_sample_error_rbf = []\n",
    "for j in C:\n",
    "    mod_res_train_rbf = SVM_analysis(train_data,train_data,1,j,2,'rbf',1,0,5)\n",
    "    mod_res_test_rbf = SVM_analysis(train_data,test_data,1,j,2,'rbf',1,0,5)\n",
    "    in_sample_error_rbf.append(mod_res_train_rbf[0])\n",
    "    out_sample_error_rbf.append(mod_res_test_rbf[0])\n",
    "    \n",
    "print(\"The minimum in sample error is observed for C= : \", C[in_sample_error_rbf.index(min(in_sample_error_rbf))])\n",
    "print(\"The minimum out of sample error is observed for C= : \", C[out_sample_error_rbf.index(min(out_sample_error_rbf))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of C as per cross-validation is:  0.01\n",
      "The average cross validation error for the best value of penalty parameter is:  0.007346153846153846\n"
     ]
    }
   ],
   "source": [
    "# Bonus: Using 7 fold cross validation to find optimal C for rbf kernel\n",
    "out_cv = cross_validation_for_C(train_data,1,C,2,100,'rbf',1,0,10,5)\n",
    "c_vec = out_cv[0]\n",
    "print(\"The best value of C as per cross-validation is: \", C[c_vec.index(max(c_vec))])\n",
    "print(\"The average cross validation error for the best value of penalty parameter is: \",out_cv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is quite important to note that there may be times when even cross validation is not sufficient to properly estimate the hyperparameters and alternate techniques must be applied to select/corroborate the same. Now, as a further experiment we try the cross validation after remvoving the previously most preferred values which was 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of C as per cross-validation is:  100\n",
      "The average cross validation error for the best value of penalty parameter is:  0.00733974358974359\n"
     ]
    }
   ],
   "source": [
    "# Performing 10 fold on a reduced C list (removing 0.01)\n",
    "C_ch = [1,100,10e4,10e6]\n",
    "out_cv = cross_validation_for_C(train_data,1,C_ch,2,100,'rbf',1,0,10,5)\n",
    "c_vec = out_cv[0]\n",
    "print(\"The best value of C as per cross-validation is: \", C_ch[c_vec.index(max(c_vec))])\n",
    "print(\"The average cross validation error for the best value of penalty parameter is: \",out_cv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that as we remove 0.01 cross validation picks up the value of the penalty parameter as suggested by the test dataset. This implies that the search space for hyperparameter optimization plays a very crucial role in identifying the optimal values of the same. Also, notice that as we do not change the folds by much the average error estimate remains constant.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
